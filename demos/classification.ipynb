{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b1b7062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path(\"..\").resolve()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier as SklearnDT\n",
    "\n",
    "from arboresque import DecisionTreeClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5178d",
   "metadata": {},
   "source": [
    "The iris dataset contains 150 samples of sepal and petal measurement data, 4 features in all, for three types of irises, Setosa, Versicolor and Virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0945f2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8 3.  1.4 0.1] 0\n",
      "[4.3 3.  1.1 0.1] 0\n",
      "[6.7 3.1 4.7 1.5] 1\n",
      "[6.7 3.1 5.6 2.4] 2\n",
      "[6.6 3.  4.4 1.4] 1\n"
     ]
    }
   ],
   "source": [
    "for i in [12, 13, 86, 140, 75]:\n",
    "    print(iris.data[i], iris.target[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d9f404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris classifier\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "clf = DecisionTreeClassifier()  # default criterion=\"gini\"\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Iris classifier\")\n",
    "print(\"Train accuracy:\", clf.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51fd46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        15\n",
      "           1       0.79      1.00      0.88        15\n",
      "           2       1.00      0.87      0.93        15\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.93      0.91      0.91        45\n",
      "weighted avg       0.93      0.91      0.91        45\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[13  2  0]\n",
      " [ 0 15  0]\n",
      " [ 0  2 13]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Classification report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion matrix:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "074a38fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion: gini\n",
      "    Train accuracy: 1.000\n",
      "    Test accuracy:  0.911\n",
      "    Time: 0.032\n",
      "    Depth: 4\n",
      "\n",
      "Criterion: entropy\n",
      "    Train accuracy: 1.000\n",
      "    Test accuracy:  0.844\n",
      "    Time: 0.025\n",
      "    Depth: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterions = [\"gini\", \"entropy\"]\n",
    "\n",
    "results = []\n",
    "for crit in criterions:\n",
    "    start = time.time()\n",
    "    clf = DecisionTreeClassifier(criterion=crit)\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    tm = end-start\n",
    "    train_acc = clf.score(X_train, y_train)\n",
    "    test_acc = clf.score(X_test, y_test)\n",
    "    print(f\"Criterion: {crit}\")\n",
    "    print(f\"    Train accuracy: {train_acc:.3f}\")\n",
    "    print(f\"    Test accuracy:  {test_acc:.3f}\")\n",
    "    print(f\"    Time: {tm:.3f}\")\n",
    "    print(f\"    Depth: {clf.get_depth()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04358ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c83e1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample true labels:  [2 2 0 0 1]\n",
      "Predicted labels:    [2 2 0 1 1]\n",
      "Predicted probs (rows):\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n",
      "Row sums (should be 1): [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "X_sample = X_test[:5]\n",
    "y_sample = y_test[:5]\n",
    "\n",
    "probs = clf.predict_proba(X_sample)\n",
    "preds = clf.predict(X_sample)\n",
    "\n",
    "print(\"Sample true labels: \", y_sample)\n",
    "print(\"Predicted labels:   \", preds)\n",
    "print(\"Predicted probs (rows):\")\n",
    "print(probs)\n",
    "print(\"Row sums (should be 1):\", probs.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2349396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 1: max_features variations\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features=None: Accuracy=0.956, Depth=6, Leaves=10\n",
      "max_features=2: Accuracy=1.000, Depth=7, Leaves=9\n",
      "max_features=0.5: Accuracy=1.000, Depth=7, Leaves=9\n",
      "max_features=sqrt: Accuracy=1.000, Depth=7, Leaves=9\n",
      "max_features=log2: Accuracy=1.000, Depth=7, Leaves=9\n",
      "\n",
      "Test 2: min_samples_split & min_samples_leaf\n",
      "\n",
      "min_split=2, min_leaf=1: Acc=0.956, Depth=6, Leaves=10\n",
      "min_split=10, min_leaf=5: Acc=1.000, Depth=4, Leaves=6\n",
      "min_split=20, min_leaf=10: Acc=0.978, Depth=3, Leaves=5\n",
      "min_split=0.1, min_leaf=0.05: Acc=1.000, Depth=4, Leaves=6\n",
      "\n",
      "Test 3: min_impurity_decrease\n",
      "\n",
      "min_impurity_decrease=0.0: Acc=0.956, Depth=6, Leaves=10\n",
      "min_impurity_decrease=0.01: Acc=0.956, Depth=6, Leaves=8\n",
      "min_impurity_decrease=0.05: Acc=0.978, Depth=2, Leaves=3\n",
      "min_impurity_decrease=0.1: Acc=0.978, Depth=2, Leaves=3\n",
      "\n",
      "Test 4: random_state reproducibility\n",
      "\n",
      "Same random_state: Predictions identical? True\n",
      "Different random_state: Predictions different? True\n",
      "\n",
      "Test 5: Categorical features\n",
      "\n",
      "With categorical feature at index 4: Accuracy=0.956\n",
      "Features after encoding: 7 (was 5)\n",
      "\n",
      "Test 6: Comparison with sklearn\n",
      "\n",
      "Arboresque implementation: Acc=0.926, Depth=4, Leaves=6\n",
      "Sklearn: Acc=0.926, Depth=3, Leaves=6\n",
      "Accuracy difference: 0.000\n",
      "\n",
      "Test 7: Edge cases\n",
      "\n",
      "Very restrictive tree: Depth=1, Leaves=2\n",
      "Max flexibility tree: Depth=4, Leaves=7\n",
      "\n",
      "Test 8: predict_proba\n",
      "\n",
      "First 5 samples:\n",
      "  Predicted class: 0, Probabilities: [1. 0. 0.]\n",
      "  Sum of probabilities: 1.000\n",
      "  Predicted class: 0, Probabilities: [1. 0. 0.]\n",
      "  Sum of probabilities: 1.000\n",
      "  Predicted class: 2, Probabilities: [0. 0. 1.]\n",
      "  Sum of probabilities: 1.000\n",
      "  Predicted class: 0, Probabilities: [1. 0. 0.]\n",
      "  Sum of probabilities: 1.000\n",
      "  Predicted class: 1, Probabilities: [0.14285714 0.85714286 0.        ]\n",
      "  Sum of probabilities: 1.000\n",
      "All tests completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print()\n",
    "print(\"Test 1: max_features variations\")\n",
    "print()\n",
    "\n",
    "for max_feat in [None, 2, 0.5, 'sqrt', 'log2']:\n",
    "    clf = DecisionTreeClassifier(max_features=max_feat, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    print(f\"max_features={max_feat}: Accuracy={acc:.3f}, Depth={clf.get_depth()}, Leaves={clf.get_n_leaves()}\")\n",
    "\n",
    "print()\n",
    "print(\"Test 2: min_samples_split & min_samples_leaf\")\n",
    "print()\n",
    "\n",
    "for min_split, min_leaf in [(2, 1), (10, 5), (20, 10), (0.1, 0.05)]:\n",
    "    clf = DecisionTreeClassifier(min_samples_split=min_split, min_samples_leaf=min_leaf, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    print(f\"min_split={min_split}, min_leaf={min_leaf}: Acc={acc:.3f}, Depth={clf.get_depth()}, Leaves={clf.get_n_leaves()}\")\n",
    "\n",
    "print()\n",
    "print(\"Test 3: min_impurity_decrease\")\n",
    "print()\n",
    "\n",
    "for min_imp in [0.0, 0.01, 0.05, 0.1]:\n",
    "    clf = DecisionTreeClassifier(min_impurity_decrease=min_imp, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    print(f\"min_impurity_decrease={min_imp}: Acc={acc:.3f}, Depth={clf.get_depth()}, Leaves={clf.get_n_leaves()}\")\n",
    "\n",
    "print()\n",
    "print(\"Test 4: random_state reproducibility\")\n",
    "print()\n",
    "\n",
    "clf1 = DecisionTreeClassifier(max_features=2, random_state=42)\n",
    "clf1.fit(X_train, y_train)\n",
    "pred1 = clf1.predict(X_test)\n",
    "\n",
    "clf2 = DecisionTreeClassifier(max_features=2, random_state=42)\n",
    "clf2.fit(X_train, y_train)\n",
    "pred2 = clf2.predict(X_test)\n",
    "\n",
    "print(f\"Same random_state: Predictions identical? {np.array_equal(pred1, pred2)}\")\n",
    "\n",
    "clf3 = DecisionTreeClassifier(max_features=2, random_state=99)\n",
    "clf3.fit(X_train, y_train)\n",
    "pred3 = clf3.predict(X_test)\n",
    "\n",
    "print(f\"Different random_state: Predictions different? {not np.array_equal(pred1, pred3)}\")\n",
    "\n",
    "print()\n",
    "print(\"Test 5: Categorical features\")\n",
    "print()\n",
    "\n",
    "X_cat = np.column_stack([X, np.random.randint(0, 3, size=len(X))])\n",
    "X_train_cat, X_test_cat, y_train, y_test = train_test_split(X_cat, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(categorical_features=[4], random_state=42)\n",
    "clf.fit(X_train_cat, y_train)\n",
    "acc = clf.score(X_test_cat, y_test)\n",
    "print(f\"With categorical feature at index 4: Accuracy={acc:.3f}\")\n",
    "print(f\"Features after encoding: {clf.n_features} (was {X_cat.shape[1]})\")\n",
    "\n",
    "print()\n",
    "print(\"Test 6: Comparison with sklearn\")\n",
    "print()\n",
    "\n",
    "X, y = wine.data, wine.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "arbor_clf = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "arbor_clf.fit(X_train, y_train)\n",
    "arbor_acc = arbor_clf.score(X_test, y_test)\n",
    "\n",
    "sklearn_clf = SklearnDT(\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "sklearn_clf.fit(X_train, y_train)\n",
    "sklearn_acc = sklearn_clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Arboresque implementation: Acc={arbor_acc:.3f}, Depth={arbor_clf.get_depth()}, Leaves={arbor_clf.get_n_leaves()}\")\n",
    "print(f\"Sklearn: Acc={sklearn_acc:.3f}, Depth={sklearn_clf.get_depth()}, Leaves={sklearn_clf.get_n_leaves()}\")\n",
    "print(f\"Accuracy difference: {abs(arbor_acc - sklearn_acc):.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"Test 7: Edge cases\")\n",
    "print()\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=1, min_samples_split=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Very restrictive tree: Depth={clf.get_depth()}, Leaves={clf.get_n_leaves()}\")\n",
    "\n",
    "clf = DecisionTreeClassifier(min_samples_leaf=1, min_samples_split=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Max flexibility tree: Depth={clf.get_depth()}, Leaves={clf.get_n_leaves()}\")\n",
    "\n",
    "print()\n",
    "print(\"Test 8: predict_proba\")\n",
    "print()\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "proba = clf.predict_proba(X_test[:5])\n",
    "preds = clf.predict(X_test[:5])\n",
    "\n",
    "print(\"First 5 samples:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Predicted class: {preds[i]}, Probabilities: {proba[i]}\")\n",
    "    print(f\"  Sum of probabilities: {proba[i].sum():.3f}\")\n",
    "\n",
    "print(\"All tests completed.\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cca56acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris sklearn DecisionTreeClassifier (default gini)\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.9777777777777777\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       0.94      1.00      0.97        15\n",
      "           2       1.00      0.93      0.97        15\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "Confusion matrix:\n",
      " [[15  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  1 14]]\n",
      "\n",
      "Criterion: gini\n",
      "  Train accuracy: 1.0\n",
      "  Test accuracy:  0.978\n",
      "  Fit time (s):   0.003558\n",
      "  Depth: 4\n",
      "\n",
      "Criterion: entropy\n",
      "  Train accuracy: 1.0\n",
      "  Test accuracy:  0.956\n",
      "  Fit time (s):   0.0\n",
      "  Depth: 7\n"
     ]
    }
   ],
   "source": [
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "clf = SklearnDT(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Iris sklearn DecisionTreeClassifier (default gini)\")\n",
    "print(\"Train accuracy:\", clf.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", clf.score(X_test, y_test))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from time import time\n",
    "\n",
    "for crit in [\"gini\", \"entropy\"]:\n",
    "    t0 = time()\n",
    "    clf = SklearnDT(criterion=crit, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    t1 = time()\n",
    "    print(f\"\\nCriterion: {crit}\")\n",
    "    print(\"  Train accuracy:\", round(clf.score(X_train, y_train), 3))\n",
    "    print(\"  Test accuracy: \", round(clf.score(X_test, y_test), 3))\n",
    "    print(\"  Fit time (s):  \", round(t1 - t0, 6))\n",
    "    print(f\"  Depth: {clf.get_depth()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec9c8a0",
   "metadata": {},
   "source": [
    "The above cells demonstrate the functionalities of the DecisionTreeClassifier in Arboresque, the next step is to see the advantage of handling categorical variables. For this I used the Adult Income dataset from the UCI Machine Learning repository, as it has a mix of categorical and numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab21c115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   age             48842 non-null  int64   \n",
      " 1   workclass       46043 non-null  category\n",
      " 2   fnlwgt          48842 non-null  int64   \n",
      " 3   education       48842 non-null  category\n",
      " 4   education-num   48842 non-null  int64   \n",
      " 5   marital-status  48842 non-null  category\n",
      " 6   occupation      46033 non-null  category\n",
      " 7   relationship    48842 non-null  category\n",
      " 8   race            48842 non-null  category\n",
      " 9   sex             48842 non-null  category\n",
      " 10  capital-gain    48842 non-null  int64   \n",
      " 11  capital-loss    48842 non-null  int64   \n",
      " 12  hours-per-week  48842 non-null  int64   \n",
      " 13  native-country  47985 non-null  category\n",
      " 14  class           48842 non-null  category\n",
      "dtypes: category(9), int64(6)\n",
      "memory usage: 2.7 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "adult = fetch_openml(\"adult\", version=2, as_frame=True)\n",
    "print(adult.frame.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31db492e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<=50K', '>50K']\n",
       "Categories (2, object): ['<=50K', '>50K']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c6112c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  education-num      marital-status  \\\n",
       "0   25    Private  226802          11th              7       Never-married   \n",
       "1   38    Private   89814       HS-grad              9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm             12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college             10  Married-civ-spouse   \n",
       "4   18        NaN  103497  Some-college             10       Never-married   \n",
       "\n",
       "          occupation relationship   race     sex  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                NaN    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country  \n",
       "0              40  United-States  \n",
       "1              50  United-States  \n",
       "2              40  United-States  \n",
       "3              40  United-States  \n",
       "4              30  United-States  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b232df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nupoo\\decision-trees\\arboresque\\tree.py:75: UserWarning: The feature at index 13 has 42 categories. The current version of this library can only handle features with at most twenty categories as categorical features.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.865\n",
      "Depth: 10, Leaves: 165\n"
     ]
    }
   ],
   "source": [
    "# from info, categorical variables are [1,3,5,6,7,8,9,13]\n",
    "X, y = adult.data, adult.target\n",
    "X_np = X.to_numpy()\n",
    "y_np = (y == '>50K').astype(int) # making it two classes, >50K 1 and <=50K 0\n",
    "cat_inds = [1,3,5,6,7,8,9,13]\n",
    "for col_idx in cat_inds:\n",
    "    mask = pd.isna(X_np[:, col_idx])\n",
    "    X_np[mask, col_idx] = 'Missing'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_size=0.3, random_state=42)\n",
    "clf = DecisionTreeClassifier(\n",
    "    categorical_features=cat_inds,\n",
    "    max_depth=10,\n",
    "    min_samples_split=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "acc = clf.score(X_test, y_test)\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"Depth: {clf.get_depth()}, Leaves: {clf.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9add6",
   "metadata": {},
   "source": [
    "With categorical indices, and some other parameters, this implementation got an accurcay of 0.861. To compare, I tested it without marking variables as categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deae9b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arboresque with categorical features marked and treated as such.\n",
      "\n",
      "Accuracy: 0.865\n",
      "Depth: 10, Leaves: 165\n",
      "\n",
      "Scikit-learn implementation\n",
      "\n",
      "Accuracy: 0.861\n",
      "Depth: 10, Leaves: 178\n",
      "\n",
      "Arboresque without handling categorical variables as such.\n",
      "\n",
      "Accuracy: 0.861\n",
      "Depth: 10, Leaves: 178\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Arboresque with categorical features marked and treated as such.\")\n",
    "print()\n",
    "clf1 = DecisionTreeClassifier(\n",
    "    categorical_features=cat_inds,\n",
    "    max_depth=10,\n",
    "    min_samples_split=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf1.fit(X_train, y_train)\n",
    "acc1 = clf1.score(X_test, y_test)\n",
    "print(f\"Accuracy: {acc1:.3f}\")\n",
    "print(f\"Depth: {clf1.get_depth()}, Leaves: {clf1.get_n_leaves()}\")\n",
    "print()\n",
    "print(\"Scikit-learn implementation\")\n",
    "print()\n",
    "X_train_sklearn = X_train.copy()\n",
    "X_test_sklearn = X_test.copy()\n",
    "\n",
    "label_encoders = {}\n",
    "for col_idx in cat_inds:\n",
    "    le = LabelEncoder()\n",
    "    X_train_sklearn[:, col_idx] = le.fit_transform(X_train_sklearn[:, col_idx])\n",
    "    X_test_sklearn[:, col_idx] = le.transform(X_test_sklearn[:, col_idx])\n",
    "\n",
    "clf2 = SklearnDT(\n",
    "    max_depth=10,\n",
    "    min_samples_split=50,\n",
    "    random_state=42\n",
    ")\n",
    "clf2.fit(X_train_sklearn, y_train)\n",
    "acc2 = clf2.score(X_test_sklearn, y_test)\n",
    "print(f\"Accuracy: {acc2:.3f}\")\n",
    "print(f\"Depth: {clf2.get_depth()}, Leaves: {clf2.get_n_leaves()}\")\n",
    "print()\n",
    "print(\"Arboresque without handling categorical variables as such.\")\n",
    "print()\n",
    "clf3 = DecisionTreeClassifier(\n",
    "    max_depth=10,\n",
    "    min_samples_split=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf3.fit(X_train_sklearn, y_train)\n",
    "acc3 = clf3.score(X_test_sklearn, y_test)\n",
    "print(f\"Accuracy: {acc3:.3f}\")\n",
    "print(f\"Depth: {clf3.get_depth()}, Leaves: {clf3.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30e49e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arboresque with categorical features marked and treated as such.\n",
      "\n",
      "Accuracy: 0.865\n",
      "Depth: 10, Leaves: 174\n",
      "Time: 177.29684400558472\n",
      "\n",
      "Arboresque without handling categorical variables as such.\n",
      "\n",
      "Accuracy: 0.860\n",
      "Depth: 10, Leaves: 201\n",
      "Time: 150.39629793167114\n"
     ]
    }
   ],
   "source": [
    "print(\"Arboresque with categorical features marked and treated as such.\")\n",
    "print()\n",
    "b1=time()\n",
    "clf1 = DecisionTreeClassifier(\n",
    "    categorical_features=cat_inds,\n",
    "    max_depth=10,\n",
    "    min_samples_split=50,\n",
    "    random_state=42,\n",
    "    max_features=0.8\n",
    ")\n",
    "clf1.fit(X_train, y_train)\n",
    "acc1 = clf1.score(X_test, y_test)\n",
    "e1=time()\n",
    "print(f\"Accuracy: {acc1:.3f}\")\n",
    "print(f\"Depth: {clf1.get_depth()}, Leaves: {clf1.get_n_leaves()}\")\n",
    "print(f\"Time: {e1-b1}\")\n",
    "print()\n",
    "\n",
    "print(\"Arboresque without handling categorical variables as such.\")\n",
    "print()\n",
    "b2=time()\n",
    "clf2 = DecisionTreeClassifier(\n",
    "    max_depth=10,\n",
    "    min_samples_split=50,\n",
    "    random_state=42,\n",
    "    max_features=0.8\n",
    ")\n",
    "\n",
    "clf2.fit(X_train_sklearn, y_train)\n",
    "acc2 = clf2.score(X_test_sklearn, y_test)\n",
    "e2=time()\n",
    "print(f\"Accuracy: {acc2:.3f}\")\n",
    "print(f\"Depth: {clf2.get_depth()}, Leaves: {clf2.get_n_leaves()}\")\n",
    "print(f\"Time: {e2-b2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
